Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Carbonell1998,
abstract = {This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization. The latter are borne out by the recent results of the SUMMAC conference in the evaluation of summarization systems. However, the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection.},
author = {Carbonell, Jaime and Goldstein, Jade},
doi = {10.1145/290941.291025},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/MMR.pdf:pdf},
isbn = {1581130155},
issn = {01635840 (ISSN)},
journal = {Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval  - SIGIR '98},
number = {June},
pages = {335--336},
pmid = {18708246},
title = {{The use of MMR, diversity-based reranking for reordering documents and producing summaries}},
url = {http://portal.acm.org/citation.cfm?doid=290941.291025},
year = {1998}
}
@article{Yang2016,
abstract = {Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.},
archivePrefix = {arXiv},
arxivId = {1707.00896},
author = {Pappas, Nikolaos and Popescu-Belis, Andrei},
doi = {10.18653/v1/N16-1174},
eprint = {1707.00896},
file = {::},
isbn = {9781941643914},
issn = {1606.02393},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1480--1489},
title = {{Multilingual Hierarchical Attention Networks for Document Classification}},
url = {http://arxiv.org/abs/1707.00896},
year = {2017}
}
@article{Li2016,
abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
archivePrefix = {arXiv},
arxivId = {1606.09282},
author = {Li, Zhizhong and Hoiem, Derek},
doi = {10.1007/978-3-319-46493-0_37},
eprint = {1606.09282},
file = {::},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
pages = {1--13},
pmid = {4520227},
title = {{Learning without Forgetting}},
url = {http://arxiv.org/abs/1606.09282},
year = {2016}
}
@article{Hill2016,
abstract = {Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.},
archivePrefix = {arXiv},
arxivId = {1602.03483},
author = {Hill, Felix and Cho, Kyunghyun and Korhonen, Anna},
doi = {10.18653/v1/N16-1162},
eprint = {1602.03483},
file = {::},
isbn = {9781941643914},
title = {{Learning Distributed Representations of Sentences from Unlabelled Data}},
url = {http://arxiv.org/abs/1602.03483},
year = {2016}
}
@article{Pappas2017,
abstract = {Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.},
archivePrefix = {arXiv},
arxivId = {1707.00896},
author = {Pappas, Nikolaos and Popescu-Belis, Andrei},
doi = {10.18653/v1/N16-1174},
eprint = {1707.00896},
file = {::},
title = {{Multilingual Hierarchical Attention Networks for Document Classification}},
url = {http://arxiv.org/abs/1707.00896},
year = {2017}
}
@article{Hulpus2013,
abstract = {Automated topic labelling brings benefits for users aiming at analysing and understanding document collections, as well as for search engines targetting at the linkage between groups of words and their inherent topics. Current ap- proaches to achieve this suffer in quality, but we argue their performances might be improved by setting the focus on the structure in the data. Building upon research for con- cept disambiguation and linking to DBpedia, we are taking a novel approach to topic labelling by making use of structured data exposed by DBpedia. We start from the hypothesis that words co-occuring in text likely refer to concepts that belong closely together in the DBpedia graph. Using graph centrality measures, we show that we are able to identify the concepts that best represent the topics. We compara- tively evaluate our graph-based approach and the standard text-based approach, on topics extracted from three corpora, based on results gathered in a crowd-sourcing experiment. Our research shows that graph-based analysis of DBpedia can achieve better results for topic labelling in terms of both precision and topic coverage.},
author = {Hulpus, Ioana and Hayes, Conor and Karnstedt, Marcel and Greene, Derek},
doi = {10.1145/2433396.2433454},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/Unsupervised graph-based topic labelling using dbpedia.pdf:pdf},
isbn = {9781450318693},
issn = {1541034X},
journal = {Proceedings of the sixth ACM international conference on Web search and data mining - WSDM '13},
pages = {465},
pmid = {22902439},
title = {{Unsupervised graph-based topic labelling using dbpedia}},
url = {http://dl.acm.org/citation.cfm?doid=2433396.2433454},
year = {2013}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {::},
journal = {NIPS'13 Proceedings of the 26th International Conference on Neural Information Processing Systems},
keywords = {word2vec},
mendeley-tags = {word2vec},
month = {oct},
pages = {3111--3119},
title = {{Distributed Representations ofWords and Phrases and their Compositionality}},
volume = {2},
year = {2013}
}
@article{Rusu2016,
abstract = {Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1610.04286},
author = {Rusu, Andrei A. and Vecerik, Mel and Roth{\"{o}}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
eprint = {1610.04286},
file = {::},
keywords = {corl,progressive networks,robot learning,sim-to-real,transfer,transfer learning},
mendeley-tags = {transfer learning},
number = {CoRL},
pages = {1--9},
title = {{Sim-to-Real Robot Learning from Pixels with Progressive Nets}},
url = {http://arxiv.org/abs/1610.04286},
year = {2016}
}
@article{Magatti2009,
abstract = {An algorithm for the automatic labeling of topics accordingly to a hierarchy is presented. Its main ingredients are a set of similarity measures and a set of topic labeling rules. The labeling rules are specifically designed to find the most agreed labels between the given topic and the hierarchy. The hierarchy is obtained from the Google Directory service, extracted via an ad-hoc developed software procedure and expanded through the use of the OpenOffice English Thesaurus. The performance of the proposed algorithm is investigated by using a document corpus consisting of 33,801 documents and a dictionary consisting of 111,795 words. The results are encouraging, while particularly interesting and significant labeling cases emerged.},
author = {Magatti, Davide and Calegari, Silvia and Ciucci, Davide and Stella, Fabio},
doi = {10.1109/ISDA.2009.165},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/automatic labeling of topic evtl hlda.pdf:pdf},
isbn = {9780769538723},
journal = {ISDA 2009 - 9th International Conference on Intelligent Systems Design and Applications},
pages = {1227--1232},
title = {{Automatic labeling of topics}},
year = {2009}
}
@article{Conneau2017a,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
archivePrefix = {arXiv},
arxivId = {1705.02364},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
doi = {10.1.1.156.2685},
eprint = {1705.02364},
file = {::},
isbn = {978-1-109-24088-7},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
url = {http://arxiv.org/abs/1705.02364},
year = {2017}
}
@article{Ringeval2017,
abstract = {The Audio/Visual Emotion Challenge and Workshop (AVEC 2017) "Real-life depression, and aaect" will be the seventh competition event aimed at comparison of multimedia processing and machine learning methods for automatic audiovisual depression and emotion analysis, with all participants competing under strictly the same conditions. .e goal of the Challenge is to provide a common benchmark test set for multimodal information processing and to bring together the depression and emotion recognition communities, as well as the audiovisual processing communities, to compare the relative merits of the various approaches to depression and emotion recognition from real-life data. .is paper presents the novelties introduced this year, the challenge guidelines, the data used, and the performance of the baseline system on the two proposed tasks: dimensional emotion recognition (time and value-continuous), and dimensional depression estimation (value-continuous).},
author = {Ringeval, Fabien and Schuller, Bj{\"{o}}rn and Valstar, Michel and Gratch, Jonathan and Cowie, Roddy and Scherer, Stefan and Mozgai, Sharon and Cummins, Nicholas and Schmii, Maximilian and Pantic, Maja},
doi = {10.1145/3133944.3133953},
file = {::},
isbn = {9781450355025},
keywords = {evaluator weighted estimator,ewe,statistics,•Computing methodologies  Biometrics,•General and reference  Performance},
mendeley-tags = {evaluator weighted estimator,ewe,statistics},
title = {{AVEC 2017-Real-life Depression, and AAect Recognition Workshop and Challenge}},
url = {http://delivery.acm.org/10.1145/3140000/3133953/p3-ringeval.pdf?ip=150.65.249.59{\&}id=3133953{\&}acc=ACTIVE SERVICE{\&}key=D2341B890AD12BFE.FC02734FB516017D.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1536202826{\_}65ce5331fb4c3a577a21529d3223b817},
year = {2017}
}
@article{Tang2014,
abstract = {We present a method that learns word em- bedding for Twitter sentiment classifica- tion in this paper. Most existing algorithm- s for learning continuous word represen- tations typically only model the syntactic context of words but ignore the sentimen- t of text. This is problematic for senti- ment analysis as they usually map word- s with similar syntactic context but oppo- site sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment- specific word embedding (SSWE), which encodes sentiment information in the con- tinuous representation of words. Specif- ically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sen- tences or tweets) in their loss function- s. To obtain large scale training corpora, we learn the sentiment-specific word em- bedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS- WE to a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the perfor- mance is further improved by concatenat- ing SSWE with existing feature set.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
doi = {10.3115/1220575.1220648},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9781937284725},
issn = {03029743},
journal = {Acl},
pages = {1555--1565},
pmid = {18487783},
title = {{Learning Sentiment-Specific Word Embedding}},
year = {2014}
}
@article{Lample2017,
abstract = {Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.},
archivePrefix = {arXiv},
arxivId = {1711.00043},
author = {Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
doi = {10.2507/26th.daaam.proceedings.070},
eprint = {1711.00043},
file = {::},
isbn = {9781479915729},
issn = {16113349},
number = {2011},
pages = {1--14},
pmid = {1000303116},
title = {{Unsupervised Machine Translation Using Monolingual Corpora Only}},
url = {http://arxiv.org/abs/1711.00043},
year = {2017}
}
@article{Lau2011,
abstract = {We propose a method for automatically la- belling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia ar- ticles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates us- ing a combination of association measures and lexical features, optionally fed into a super- vised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.},
author = {Lau, Jey Han and Grieser, Karl and Newman, David and Baldwin, Timothy},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/automatic topic labeling of topic models.pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics},
pages = {1536--1545},
title = {{Automatic Labelling of Topic Models}},
year = {2011}
}
@phdthesis{Widmer,
abstract = {Comment sections of editorial news sites, blogs, and forums are becoming a increasingly popular place for users and consumers to exchange their opinions. In this thesis we perform topic modeling with Latent Dirichlet Allocation (LDA) and Non-negative Matrix factorization to identify the main topics of German and U.S. discussions regarding organic food. With the help of a domain expert the identified topics were compared to consumer responses from qualitative surveys. Our results show that the issues that are important to the consumers when making consumption decisions are also expressed in online discussions. Further, we show that topic modeling can be used to identify events and trends that had an significant impact on online discussions.},
author = {Widmer, Christian},
file = {::},
school = {Technische Universit{\"{a}}t M{\"{u}}nchen},
title = {{Topic Modeling for Opinion Mining}}
}
@article{Blei2004,
abstract = {We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, gen- erating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior al- lows arbitrarily large branching factors and readily accommodates grow- ing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.},
archivePrefix = {arXiv},
arxivId = {arXiv:0710.0845v2},
author = {Blei, D. and Griffiths, T.L. and Jordan, M.I. and Tenenbaum, J.B.},
doi = {10.1016/0169-023X(89)90004-9},
eprint = {arXiv:0710.0845v2},
isbn = {0262201526},
issn = {0169023X},
journal = {Advances in neural information processing systems},
pmid = {25122015},
title = {{Hierarchical topic models and the nested Chinese restaurant process}},
year = {2004}
}
@misc{FrancoiseBeaufays2017,
author = {{Francoise Beaufays}, Google},
title = {{I/O '17 Guide - Interview with}},
url = {https://www.youtube.com/watch?v=jUFetIK1whg},
year = {2017}
}
@article{Aghajanyan2018a,
abstract = {When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in,even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics and formalizing Universal Grammar as an optimization process (Chomsky, 2014; Montague, 1970). We demonstrate the capabilities of these representations by showing that the models trained on a single language using language agnostic representations achieve very similar accuracies in other languages.},
archivePrefix = {arXiv},
arxivId = {1809.08510},
author = {Aghajanyan, Armen and Song, Xia and Tiwary, Saurabh},
eprint = {1809.08510},
file = {::},
pages = {1--10},
title = {{Towards Language Agnostic Universal Representations}},
url = {http://arxiv.org/abs/1809.08510},
year = {2018}
}
@article{Kou2015,
abstract = {Before deciding to buy a product, many people tend to consult others' opinions on it. Web provides a perfect platform which$\backslash$n one can get information to find out the advantages and disadvantages of the product of his interest. How to automatically$\backslash$n manage the numerous opinionated documents and then to give suggestions to the potential customers is becoming a research hotspot$\backslash$n recently. Constructing a sentiment resource is one of the vital elements of opinion finding and polarity analysis tasks. For$\backslash$n a specific domain, the sentiment resource can be regarded as a dictionary, which contains a list of product feature words$\backslash$n and several opinion words with sentiment polarity for each feature word. This paper proposes an automatic algorithm to extraction$\backslash$n feature words and opinion words for the sentiment resource. We mine the feature words and opinion words from the comments$\backslash$n on the Web with both NLP technique and statistical method. Left context entropy is proposed to extract unknown feature words;$\backslash$n Adjective rules and background corpus are taken into consideration in the algorithm. Experimental results show the effectiveness$\backslash$n of the proposed automatic sentiment resource construction approach. The proposed method that combines NLP and statistical$\backslash$n techniques is better than using only NLP-based technique. Although the experiment is built on mobile telephone comments in$\backslash$n Chinese, the algorithm is domain independent.},
author = {Kou, Wanqiu and Li, Fang and Baldwin, Timothy},
doi = {10.1007/978-3-319-28940-3_20},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/Automatic Labelling of Topic Models using Word ... - Semantic Scholar.pdf:pdf},
isbn = {9783319289397},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Letter trigram vectors,Topic labelling,Word vectors},
number = {1},
pages = {253--264},
title = {{Automatic labelling of topic models using word vectors and letter trigram vectors}},
volume = {9460},
year = {2015}
}
@article{Yang2016a,
abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
archivePrefix = {arXiv},
arxivId = {1606.02393},
author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
doi = {10.18653/v1/N16-1174},
eprint = {1606.02393},
isbn = {9781941643914},
issn = {1606.02393},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1480--1489},
title = {{Hierarchical Attention Networks for Document Classification}},
url = {http://aclweb.org/anthology/N16-1174},
year = {2016}
}
@article{Akhundov2018,
abstract = {We take a practical approach to solving sequence labeling problem assuming unavailability of domain expertise and scarcity of informational and computational resources. To this end, we utilize a universal end-to-end Bi-LSTM-based neural sequence labeling model applicable to a wide range of NLP tasks and languages. The model combines morphological, semantic, and structural cues extracted from data to arrive at informed predictions. The model's performance is evaluated on eight benchmark datasets (covering three tasks: POS-tagging, NER, and Chunking, and four languages: English, German, Dutch, and Spanish). We observe state-of-the-art results on four of them: CoNLL-2012 (English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus (German POS-tagging), and competitive performance on the rest.},
archivePrefix = {arXiv},
arxivId = {1808.03926},
author = {Akhundov, Adnan and Trautmann, Dietrich and Groh, Georg},
eprint = {1808.03926},
file = {::},
title = {{Sequence Labeling: A Practical Approach}},
url = {http://arxiv.org/abs/1808.03926},
year = {2018}
}
@misc{Statista2014a,
author = {AGOF},
file = {:D$\backslash$:/Bachelorarbeit/Paper/AGOF nettoreichweite--ab-16-jahre--der-nachrichtenwebsites-im-september-2018.pdf:pdf},
number = {September},
pages = {2018},
title = {{Nettoreichweite der Top 15 Nachrichtenseiten (ab 14 Jahre) im November 2014 in Unique Usern (in Millionen)}},
url = {http://de.statista.com/statistik/daten/studie/165258/umfrage/reichweite-der-meistbesuchten-nachrichtenwebsites/},
volume = {2018},
year = {2014}
}
@inproceedings{Xu2017,
abstract = {Although semi-supervised variational autoencoder (SemiVAE) works in image classification task, it fails in text classification task if using vanilla LSTM as its decoder. From a perspective of reinforcement learning, it is verified that the decoder's ca- pability to distinguish between different categorical labels is essential. Therefore, Semi-supervised Sequential Variational Autoencoder (SSVAE) is proposed, which increases the capa- bility by feeding label into its decoder RNN at each time-step. Two specific decoder structures are investigated and both of them are verified to be effective. Besides, in order to reduce the computational complexity in training, a novel optimization method is proposed, which estimates the gradient of the unla- beled objective function by sampling, along with two variance reduction techniques. Experimental results on Large Movie Review Dataset (IMDB) and AG's News corpus show that the proposed approach significantly improves the classifica- tion accuracy compared with pure-supervised classifiers, and achieves competitive performance against previous advanced methods. State-of-the-art results can be obtained by integrating other pretraining-based methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.02514v3},
author = {Xu, Weidi and Sun, Haoze and Deng, Chao and Tan, Ying},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
doi = {10.1051/0004-6361/201527329},
eprint = {arXiv:1603.02514v3},
file = {::},
isbn = {9781493903801},
issn = {19406045},
keywords = {Natural Language Processing and Machine Learning},
pmid = {25246403},
title = {{Variational Autoencoder for Semi-Supervised Text Classification}},
year = {2017}
}
@article{Bhatia2016,
abstract = {Topics generated by topic models are typically represented as list of terms. To reduce the cognitive overhead of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels.},
archivePrefix = {arXiv},
arxivId = {1612.05340},
author = {Bhatia, Shraey and Lau, Jey Han and Baldwin, Timothy},
eprint = {1612.05340},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/Automatic labeling of semantic roles.pdf:pdf},
number = {1},
pages = {953--963},
title = {{Automatic Labelling of Topics with Neural Embeddings}},
url = {http://arxiv.org/abs/1612.05340},
year = {2016}
}
@book{Manning2008,
abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar and Schutze, Hinrich},
doi = {10.1017/CBO9780511809071},
eprint = {0521865719 9780521865715},
file = {:D$\backslash$:/Bachelorarbeit/Paper/Introduction to Information Retrieval.pdf:pdf},
isbn = {9780511809071},
issn = {08912017},
pmid = {3393},
title = {{Introduction to Information Retrieval}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511809071},
year = {2008}
}
@phdthesis{Ayyad2018,
author = {Ayyad, Ahmed},
file = {::},
title = {{Aspect-based Sentiment Analysis}},
year = {2018}
}
@article{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
archivePrefix = {arXiv},
arxivId = {1606.04671},
author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
eprint = {1606.04671},
file = {::},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@article{Schwenk2017,
abstract = {In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics. We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.},
archivePrefix = {arXiv},
arxivId = {1704.04154},
author = {Schwenk, Holger and Douze, Matthijs},
doi = {10.18653/v1/W17-2619},
eprint = {1704.04154},
file = {::},
isbn = {9781450304610},
pages = {157--167},
title = {{Learning Joint Multilingual Sentence Representations with Neural Machine Translation}},
url = {http://arxiv.org/abs/1704.04154},
year = {2017}
}
@article{Ayyad2018a,
author = {Ayyad, Ahmed},
file = {::},
title = {{Aspect-based Sentiment Analysis}},
year = {2018}
}
@article{Goldberg2015,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
archivePrefix = {arXiv},
arxivId = {1510.00726},
author = {Goldberg, Yoav},
doi = {10.1613/jair.4992},
eprint = {1510.00726},
file = {::},
issn = {1076-9757},
pages = {1--75},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://arxiv.org/abs/1510.00726},
year = {2015}
}
@article{Popescul2000,
abstract = {Automatically labeling document clusters with words which indicate their topics is difficult to do well. The most commonly used method, labeling with the most frequent words in the clusters, ends up using many words that are virtually void of descriptive power even after traditional stop words are removed. Another method, labeling with the most predictive words, often includes rather obscure words. We present two methods of labeling document clusters motivated by the model that words are generated by a hierarchy of mixture components of varying generality. The first method assumes existence of a document hierarchy (manually constructed or resulting from a hierarchical clustering algorithm) and uses a X2 test of significance to detect different word usage across categories in the hierarchy. The second method selects words which both occur frequently in a cluster and effectively discriminate the given cluster from the other clusters. We compare these methods on abstracts of documents selected from a subset of the hierarchy of the Cora search engine for computer science research papers. Labels produced by our methods showed superior results to the commonly employed methods.},
author = {Popescul, Alexandrin and Ungar, Lyle H},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/Automatic labeling of document clusters.pdf:pdf},
journal = {Technical Report},
pages = {1--16},
title = {{Automatic Labeling of Document Clusters}},
year = {2000}
}
@article{Liu2017,
abstract = {Distant-supervised relation extraction in-evitably suffers from wrong labeling prob-lems because it heuristically labels rela-tional facts with knowledge bases. Pre-vious sentence level denoise models don't achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable dur-ing training. To this end, we introduce an entity-pair level denoise method which ex-ploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms the state-of-the-art systems.},
author = {Liu, Tianyu and Wang, Kexiang and Chang, Baobao and Sui, Zhifang},
doi = {10.18653/v1/D17-1189},
file = {::},
journal = {Proceedings of the 2017 Conference on Empirical Methods in Natural
          Language Processing},
pages = {1790--1795},
title = {{A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction}},
url = {http://aclweb.org/anthology/D17-1189},
year = {2017}
}
@article{He2017,
abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
doi = {10.18653/v1/P17-1036},
file = {::},
isbn = {9781945626753},
journal = {Proceedings of the 55th Annual Meeting of the Association for
          Computational Linguistics (Volume 1: Long Papers)},
pages = {388--397},
title = {{An Unsupervised Neural Attention Model for Aspect Extraction}},
url = {http://aclweb.org/anthology/P17-1036},
year = {2017}
}
@article{Howard2018,
abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
archivePrefix = {arXiv},
arxivId = {1801.06146},
author = {Howard, Jeremy and Ruder, Sebastian},
doi = {arXiv:1801.06146v3},
eprint = {1801.06146},
file = {::},
keywords = {transfer learning},
mendeley-tags = {transfer learning},
title = {{Universal Language Model Fine-tuning for Text Classification}},
url = {http://arxiv.org/abs/1801.06146},
year = {2018}
}
@article{Jurafsky2009,
abstract = {Dave Bowman: Open the pod bay doors, HAL. HAL: Im sorry Dave, Im afraid I cant do that. Kubrick, Stanley Clarke, Arthur C},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jurafsky, Daniel and Martin, James H},
doi = {10.1162/089120100750105975},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/Bachelorarbeit/Paper/Speech and language processing.pdf:pdf},
isbn = {0130950696},
issn = {08912017},
journal = {Speech and Language Processing An Introduction to Natural Language Processing Computational Linguistics and Speech Recognition},
pages = {0--934},
pmid = {19878769},
title = {{Speech and Language Processing}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/089120100750105975},
volume = {21},
year = {2009}
}
@article{Musat2012,
abstract = {This work outlines a novel system that automatically extracts conceptual labels for statistically obtained topics. By creating a projection of the topic, which is a distribution over all the vocabulary words, over the WordNet ontology we succeed in associating concepts to the said groups of words. The most important contributions of this paper are connected to the validation of the role of these concepts as topical labels and the determination of correlations that emerge between the utility of these labels and the strength of the relation between the concepts and the topics.},
author = {Muşat, Claudiu Cristian and Trǎuşan-Matu, Ştefan and Velcin, Julien and Rizoiu, Marian-Andrei},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/Wordnet/automatic extraction of conceptual lables from topic models.pdf:pdf},
issn = {1454234X},
journal = {UPB Scientific Bulletin, Series C: Electrical Engineering},
keywords = {Conceptual processing,Labels,Topic models,WordNet},
number = {2},
pages = {57--68},
title = {{Automatic extraction of conceptual labels from topic models}},
volume = {74},
year = {2012}
}
@article{Conneau2017,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
archivePrefix = {arXiv},
arxivId = {1705.02364},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
doi = {10.1.1.156.2685},
eprint = {1705.02364},
file = {::},
isbn = {978-1-109-24088-7},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
url = {http://arxiv.org/abs/1705.02364},
year = {2017}
}
@article{Emnlp2018,
abstract = {absa;},
archivePrefix = {arXiv},
arxivId = {1808.09238},
author = {{Martin Schmitt}},
eprint = {1808.09238},
file = {::},
pages = {1--5},
title = {{Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks}},
year = {2018}
}
@article{Berard2016,
abstract = {We present MultiVec, a new toolkit for computing continuous representations for text at different granularity levels (word-level or sequences of words). MultiVec includes word2vec's features, paragraph vector (batch and online) and bivec for bilingual distributed representations. MultiVec also includes different distance measures between words and sequences of words. The toolkit is written in C++ and is aimed at being fast (in the same order of magnitude as word2vec), easy to use, and easy to extend. It has been evaluated on several NLP tasks: the analogical reasoning task, sentiment analysis, and crosslingual document classification.},
author = {B{\'{e}}rard, Alexandre and Servan, Christophe and Pietquin, Olivier and Besacier, Laurent},
file = {::},
isbn = {9782951740891},
journal = {The 10th edition of the Language Resources and Evaluation Conference (LREC 2016)},
keywords = {bilingual word embeddings,crosslingual document classification,paragraph vector,word embeddings},
number = {1},
pages = {1--5},
title = {{MultiVec: a Multilingual and Multilevel Representation Learning Toolkit for NLP}},
volume = {1},
year = {2016}
}
@phdthesis{Wittmann2018,
abstract = {For the past few decades, relational databases have been the default choice for data storage, especially for enterprise applications. Currently, many other database technologies are grabbing more attention due to their high performance, scalability, and availability options, such as Not Only SQL (NoSQL) technologies. Choosing the right database technology for applications among a plethora of database options is a challenging task. This research aims to provide a systematic and experimental evaluation of four NoSQL databases across the spectrum of different NoSQL categories. The investigated databases are Redis, MongoDB, Neo4j, and Cassandra. We study multiple aspects such as the database transaction support, query options, data layout, scalability, availability, security, and durability. Besides, we analyze the data modelling of each database using a data model from the Transaction Processing Council Ad-hoc (TPCH) benchmark. Furthermore, we evaluate the query capabilities of each database using three complex queries from the same benchmark. Based on the examination, we capture the strengths and weaknesses each database has. Finally, we present a set of factors that influence the adoption of a NoSQL solution. These factors assist software architects and developers to choose the proper database technology for their specific application needs.},
author = {Wittmann, Elisabeth},
file = {::},
isbn = {1253978748590},
title = {{Transfer Learning for Emotion Recognition on Multimodal Data from Children with Autism Spectrum Condition}},
url = {http://www.ii.uib.no/publikasjoner/texrap/pdf/2008-367.pdf},
year = {2018}
}
@article{Mitchell2010,
abstract = {Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task.},
author = {Mitchell, Jeff and Lapata, Mirella},
doi = {10.1111/j.1551-6709.2010.01106.x},
file = {::},
isbn = {1551-6709 (Electronic)$\backslash$r0364-0213 (Linking)},
issn = {03640213},
journal = {Cognitive Science},
keywords = {compositionality,connectionism,distributional models,meaning representations,phrase similarity,semantic spaces},
number = {8},
pages = {1388--1429},
pmid = {21564253},
title = {{Composition in Distributional Models of Semantics}},
url = {http://doi.wiley.com/10.1111/j.1551-6709.2010.01106.x},
volume = {34},
year = {2010}
}
@article{Radford2017,
abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
archivePrefix = {arXiv},
arxivId = {1704.01444},
author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
eprint = {1704.01444},
file = {::},
title = {{Learning to Generate Reviews and Discovering Sentiment}},
url = {http://arxiv.org/abs/1704.01444},
year = {2017}
}
@misc{IVW2018,
author = {IVW},
file = {:D$\backslash$:/Bachelorarbeit/Paper/IVWauflage-der-ueberregionalen-tageszeitungen-im-3-quartal-2018.pdf:pdf},
pages = {2018},
title = {{Verkaufte Auflage der {\"{u}}berregionalen Tageszeitungen in Deutschland im 3 . Quartal 2018}},
year = {2018}
}
@article{Joulin2018,
abstract = {Continuous word representations, learned on different languages, can be aligned with remarkable precision. Using a small bilingual lexicon as training data, learning the linear transformation is often formulated as a regression problem using the square loss. The obtained mapping is known to suffer from the hubness problem, when used for retrieval tasks (e.g. for word translation). To address this issue, we propose to use a retrieval criterion instead of the square loss for learning the mapping. We evaluate our method on word translation, showing that our loss function leads to state-of-the-art results, with the biggest improvements observed for distant language pairs such as English-Chinese.},
archivePrefix = {arXiv},
arxivId = {1804.07745},
author = {Joulin, Armand and Bojanowski, Piotr and Mikolov, Tomas and Grave, Edouard},
eprint = {1804.07745},
file = {::},
title = {{Improving Supervised Bilingual Mapping of Word Embeddings}},
url = {http://arxiv.org/abs/1804.07745},
year = {2018}
}
@article{He2018,
abstract = {Aspect-level sentiment classification aims to determine the sentiment polarity of a review sentence towards an opinion target. A sentence could contain multiple sentiment-target pairs; thus the main challenge of this task is to separate different opinion contexts for different targets. To this end, attention mechanism has played an important role in previous state-of-the-art neural models. The mechanism is able to capture the importance of each context word towards a target by modeling their semantic associations. We build upon this line of research and propose two novel approaches for improving the effectiveness of attention. First, we propose a method for target representation that better captures the semantic meaning of the opinion target. Second, we introduce an attention model that incorporates syntactic information into the attention mechanism. We experiment on attention-based LSTM (Long Short-Term Memory) models using the datasets from SemEval 2014, 2015, and 2016. The experimental results show that the conventional attention-based LSTM can be substantially improved by incorporating the two approaches.},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
file = {::},
pages = {1121--1131},
title = {{Effective Attention Modeling for Aspect-Level Sentiment Classification}},
url = {http://aclweb.org/anthology/C18-1096},
year = {2018}
}
@article{Mei2007,
abstract = {Multinomial distributions over words are frequently used to model topics in text collections. A common, major chal-lenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automat-ically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information be-tween a label and a topic model. Experiments with user study have been done on two text data sets with different genres. The results show that the proposed labeling meth-ods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.},
author = {Mei, Qiaozhu and Shen, Xuehua and Zhai, ChengXiang},
doi = {10.1145/1281192.1281246},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/automaticLabeling Mai.pdf:pdf},
isbn = {9781595936097},
issn = {9781595936097},
journal = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD '07},
keywords = {1 is a topic,col-,database literature,lection of abstracts of,left side of table,model extracted from a,multinomial distribu-,statistical topic models,this model gives,tion,topic model labeling},
number = {January 2007},
pages = {490},
title = {{Automatic labeling of multinomial topic models}},
url = {http://portal.acm.org/citation.cfm?doid=1281192.1281246},
year = {2007}
}
@article{Stanic2009,
abstract = {This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization. The latter are borne out by the recent results of the SUMMAC conference in the evaluation of summarization systems. However, the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection.},
author = {Stanic, Elena and Lipavsky, Corina},
doi = {10.1145/290941.291025},
file = {:D$\backslash$:/Bachelorarbeit/Paper/MMR.pdf:pdf},
isbn = {1581130155},
issn = {01635840 (ISSN)},
journal = {Education},
pages = {599},
title = {{Atlas of grapich designers}},
year = {2009}
}
@article{McCann2017,
abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
archivePrefix = {arXiv},
arxivId = {1708.00107},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
eprint = {1708.00107},
file = {::},
issn = {10495258},
number = {Nips},
pages = {1--12},
title = {{Learned in Translation: Contextualized Word Vectors}},
url = {http://arxiv.org/abs/1708.00107},
year = {2017}
}
@article{Zhao2011,
abstract = {Summarizing and analyzing Twitter content is an important and challenging task. In this paper, we propose to extract topical keyphrases as one way to summarize Twitter. We propose a context-sensitive topical PageRank method for keyword ranking and a probabilistic scoring function that considers both relevance and interestingness of keyphrases for keyphrase ranking. We evaluate our proposed methods on a large Twitter data set. Experiments show that these methods are very effective for topical keyphrase extraction.},
author = {Zhao, Wayne Xin and Jiang, Jing and He, Jing and Song, Yang and Achananuparp, Palakorn and Lim, Ee-Peng and Li, Xiaoming},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/FINAL/Topical Keyphrase Extraction from Twitter.pdf:pdf},
isbn = {978-1-932432-87-9},
issn = {1909-230X},
journal = {HLT '11 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
pages = {379--388},
title = {{Topical keyphrase extraction from Twitter}},
url = {http://dl.acm.org/citation.cfm?id=2002472.2002521},
year = {2011}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {::},
isbn = {1532-4435},
issn = {15324435},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Allahyari2015,
abstract = {—Topic models, which frequently represent topics as multinomial distributions over words, have been extensively used for discovering latent topics in text corpora. Topic labeling, which aims to assign meaningful labels for discovered topics, has recently gained significant attention. In this paper, we argue that the quality of topic labeling can be improved by considering ontology concepts rather than words alone, in contrast to previous works in this area, which usually represent topics via groups of words selected from topics. We have created: (1) a topic model that integrates ontological concepts with topic models in a single framework, where each topic and each concept are represented as a multinomial distribution over concepts and over words, respectively, and (2) a topic labeling method based on the ontological meaning of the concepts included in the discovered topics. In selecting the best topic labels, we rely on the semantic relatedness of the concepts and their ontological classifications. The results of our experiments conducted on two different data sets show that introducing concepts as additional, richer features between topics and words and describing topics in terms of concepts offers an effective method for generating meaningful labels for the discovered topics.},
author = {Allahyari, Mehdi and Kochut, Krys},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/automatic topic labeling using ontology based topic models.pdf:pdf},
keywords = {DBpedia ontology,topic model labeling,topic modeling,—Statistical learning},
title = {{Automatic Topic Labeling using Ontology-based Topic Models}},
year = {2015}
}
@article{He2018a,
abstract = {Attention-based long short-term memory (LSTM) networks have proven to be useful in aspect-level sentiment classification. However, due to the difficulties in annotating aspect-level data, existing public datasets for this task are all relatively small, which largely limits the effectiveness of those neural models. In this paper, we explore two approaches that transfer knowledge from document- level data, which is much less expensive to obtain, to improve the performance of aspect-level sentiment classification. We demonstrate the effectiveness of our approaches on 4 public datasets from SemEval 2014, 2015, and 2016, and we show that attention-based LSTM benefits from document-level knowledge in multiple ways.},
archivePrefix = {arXiv},
arxivId = {1806.04346},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
eprint = {1806.04346},
file = {::},
title = {{Exploiting Document Knowledge for Aspect-level Sentiment Classification}},
url = {http://arxiv.org/abs/1806.04346},
year = {2018}
}
@article{Aghajanyan2018,
abstract = {When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in,even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics and formalizing Universal Grammar as an optimization process (Chomsky, 2014; Montague, 1970). We demonstrate the capabilities of these representations by showing that the models trained on a single language using language agnostic representations achieve very similar accuracies in other languages.},
archivePrefix = {arXiv},
arxivId = {1809.08510},
author = {Aghajanyan, Armen and Song, Xia and Tiwary, Saurabh},
eprint = {1809.08510},
pages = {1--10},
title = {{Towards Language Agnostic Universal Representations}},
url = {http://arxiv.org/abs/1809.08510},
year = {2018}
}
@article{Pham2015,
abstract = {We propose a novel approach to learning dis-tributed representations of variable-length text sequences in multiple languages simultane-ously. Unlike previous work which often de-rive representations of multi-word sequences as weighted sums of individual word vec-tors, our model learns distributed representa-tions for phrases and sentences as a whole. Our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efficiently encode meaning-equivalent text sequences of multi-ple languages in the same semantic space. Our learned embeddings achieve state-of-the-art performance in the often used crosslingual document classification task (CLDC) with an accuracy of 92.7 for English to German and 91.5 for German to English. By learning text sequence representations as a whole, our model performs equally well in both classifi-cation directions in the CLDC task in which past work did not achieve.},
author = {Pham, Hieu and Luong, Minh-Thang and Manning, Christopher D.},
file = {::},
journal = {Workshop on Vector Modeling for NLP},
pages = {88--94},
title = {{Learning Distributed Representations for Multilingual Text Sequences}},
year = {2015}
}
@article{Zanzotto2010,
abstract = {In distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored. In this paper we propose a novel approach to estimate parameters for a class of compositional distributional models: the additive models. Our approach leverages on two main ideas. Firstly, a novel idea for extracting compositional distributional semantics examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics.},
author = {Zanzotto, F M and Korkontzelos, I and Fallucchi{\ldots}, F},
file = {::},
journal = {Proceedings of the 23rd {\ldots}},
number = {August},
pages = {1263--1271},
title = {{Estimating linear models for compositional distributional semantics}},
url = {http://dl.acm.org/citation.cfm?id=1873923{\%}5Cnpapers://5860649b-6292-421d-b3aa-1b17a5231ec5/Paper/p139382},
volume = {1},
year = {2010}
}
@article{Bhatia2016a,
abstract = {Topics generated by topic models are typically represented as list of terms. To reduce the cognitive overhead of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels.},
archivePrefix = {arXiv},
arxivId = {1612.05340},
author = {Bhatia, Shraey and Lau, Jey Han and Baldwin, Timothy},
eprint = {1612.05340},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/Automatic labeling of semantic roles.pdf:pdf},
number = {1},
pages = {953--963},
title = {{Automatic Labelling of Topics with Neural Embeddings}},
url = {http://arxiv.org/abs/1612.05340},
year = {2016}
}
@article{Newman2010,
abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on point- wise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
author = {Newman, David and Lau, Jh and Grieser, Karl and Baldwin, Timothy},
doi = {10.3115/1220175.1220274},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/Wordnet/Automatic Evaluation of Topic Coherence.pdf:pdf},
isbn = {1932432655},
journal = {{\ldots} Language Technologies: The {\ldots}},
number = {June},
pages = {100--108},
title = {{Automatic evaluation of topic coherence}},
url = {http://dl.acm.org/citation.cfm?id=1858011},
year = {2010}
}
@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {::},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@article{Pham2015a,
abstract = {We propose a novel approach to learning dis-tributed representations of variable-length text sequences in multiple languages simultane-ously. Unlike previous work which often de-rive representations of multi-word sequences as weighted sums of individual word vec-tors, our model learns distributed representa-tions for phrases and sentences as a whole. Our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efficiently encode meaning-equivalent text sequences of multi-ple languages in the same semantic space. Our learned embeddings achieve state-of-the-art performance in the often used crosslingual document classification task (CLDC) with an accuracy of 92.7 for English to German and 91.5 for German to English. By learning text sequence representations as a whole, our model performs equally well in both classifi-cation directions in the CLDC task in which past work did not achieve.},
author = {Pham, Hieu and Luong, Minh-Thang and Manning, Christopher D.},
file = {::},
journal = {Workshop on Vector Modeling for NLP},
pages = {88--94},
title = {{Learning Distributed Representations for Multilingual Text Sequences}},
year = {2015}
}
@article{Guo2018,
abstract = {This paper presents an effective approach for parallel corpus mining using bilingual sentence embeddings. Our embedding models are trained to produce similar representations exclusively for bilingual sentence pairs that are translations of each other. This is achieved using a novel training method that introduces hard negatives consisting of sentences that are not translations but that have some degree of semantic similarity. The quality of the resulting embeddings are evaluated on parallel corpus reconstruction and by assessing machine translation systems trained on gold vs. mined sentence pairs. We find that the sentence embeddings can be used to reconstruct the United Nations Parallel Corpus at the sentence level with a precision of 48.9{\%} for en-fr and 54.9{\%} for en-es. When adapted to document level matching, we achieve a parallel document matching accuracy that is comparable to the significantly more computationally intensive approach of [Jakob 2010]. Using reconstructed parallel data, we are able to train NMT models that perform nearly as well as models trained on the original data (within 1-2 BLEU).},
archivePrefix = {arXiv},
arxivId = {1807.11906},
author = {Guo, Mandy and Shen, Qinlan and Yang, Yinfei and Ge, Heming and Cer, Daniel and Abrego, Gustavo Hernandez and Stevens, Keith and Constant, Noah and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
doi = {arXiv:1807.11906v2},
eprint = {1807.11906},
file = {::},
issn = {1938-7228},
title = {{Effective Parallel Corpus Mining using Bilingual Sentence Embeddings}},
url = {http://arxiv.org/abs/1807.11906},
year = {2018}
}
@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
author = {Miller, George A.},
doi = {10.1145/219717.219748},
file = {:D$\backslash$:/Bachelorarbeit/Paper/automatic TM/Wordnet/Wordnet.pdf:pdf},
isbn = {1558602720},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
pmid = {17081734},
title = {{WordNet: a lexical database for English}},
url = {http://portal.acm.org/citation.cfm?doid=219717.219748},
volume = {38},
year = {1995}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
doi = {10.1145/2740908.2742760},
eprint = {1405.4053},
file = {::},
isbn = {9781634393973},
issn = {10495258},
pmid = {9377276},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Rivard2000,
author = {Rivard, By Hugues and Member, Associate and Fenves, Steven J and Member, Honorary},
file = {::},
keywords = {transfer learning},
mendeley-tags = {transfer learning},
number = {July},
pages = {151--159},
title = {{REPRESENTATION STABILITY AS A REGULARIZER FOR IMPROVED TEXT ANALYTICS TRANSFER LEARNING}},
volume = {14},
year = {2000}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0262661160},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@phdthesis{Rajat2017,
abstract = {Distant supervision is an efficient approach to automatically label data for relation extraction task. This work introduces a novel approach for the task of relation extraction using distant supervision. Building upon the success of deep learning, our approach utilizes bidirectional LSTMs, multi-level attention network and pre-trained word embeddings to solve the problem. By incorporating several best practices such as Xavier initialization, learning rate decay, dropout, etc. our model was able to achieve state-of-the-art performance on NYT 2010 relation extraction dataset. Moreover, the work discusses problems and possible solutions of applying relation extraction on real world dataset.},
author = {Rajat, Jain},
file = {::},
isbn = {1253978748590},
title = {{Relation Extraction and Classification using Machine Learning}},
url = {http://www.ii.uib.no/publikasjoner/texrap/pdf/2008-367.pdf},
year = {2017}
}
@article{Conneau2017b,
abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
archivePrefix = {arXiv},
arxivId = {1710.04087},
author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou, Herv{\'{e}}},
doi = {http://dx.doi.org/10.1111/j.1540-4560.2007.00543.x},
eprint = {1710.04087},
file = {::},
issn = {0022-4537 1540-4560},
pages = {1--14},
title = {{Word Translation Without Parallel Data}},
url = {http://arxiv.org/abs/1710.04087},
year = {2017}
}
@article{Epresentations2018,
abstract = {We present a novel multi-task training approach to learning multilingual dis-tributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. We construct sentence embeddings by processing word embeddings with an LSTM and by taking an average of the outputs. Our architec-ture can transparently use both monolingual and sentence aligned bilingual cor-pora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows com-petitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a low-resource scenario.},
author = {Epresentations, R},
file = {::},
journal = {Iclr2018},
pages = {1--9},
title = {{Multitask Learning of Multilingual Sentence Representations}},
year = {2018}
}
@article{Salton1975,
author = {Salton, G and Wong, A and Yang, C S},
doi = {10.1145/361219.361220},
file = {:D$\backslash$:/Bachelorarbeit/Paper/A vector space model.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
keywords = {and phrases,automatic indexing,automatic information,content analysis,document,retrieval},
number = {11},
pmid = {15142973},
title = {{1975.A vector space model for automatic indexing.pdf}},
volume = {18},
year = {1975}
}
@misc{AGOF2018,
author = {AGOF},
file = {:D$\backslash$:/Bachelorarbeit/Paper/AGOF nettoreichweite--ab-16-jahre--der-nachrichtenwebsites-im-september-2018.pdf:pdf},
number = {September},
title = {{Nettoreichweite der Top 15 Nachrichtenseiten (ab 14 Jahre) im November 2014 in Unique Usern (in Millionen)}},
url = {http://de.statista.com/statistik/daten/studie/165258/umfrage/reichweite-der-meistbesuchten-nachrichtenwebsites/},
volume = {2018},
year = {2018}
}
@misc{IVW2018,
author = {IVW},
file = {:D$\backslash$:/Bachelorarbeit/Paper/IVWauflage-der-ueberregionalen-tageszeitungen-im-3-quartal-2018.pdf:pdf},
pages = {2018},
title = {{Verkaufte Auflage der {\"{u}}berregionalen Tageszeitungen in Deutschland im 3 . Quartal 2018}},
year = {2018}
}
@article{Blei2004a,
abstract = {We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, gen- erating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior al- lows arbitrarily large branching factors and readily accommodates grow- ing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.},
archivePrefix = {arXiv},
arxivId = {arXiv:0710.0845v2},
author = {Blei, D. and Griffiths, T.L. and Jordan, M.I. and Tenenbaum, J.B.},
doi = {10.1016/0169-023X(89)90004-9},
eprint = {arXiv:0710.0845v2},
file = {:D$\backslash$:/Bachelorarbeit/Paper/HTM/hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf:pdf},
isbn = {0262201526},
issn = {0169023X},
journal = {Advances in neural information processing systems},
pmid = {25122015},
title = {{Hierarchical topic models and the nested Chinese restaurant process}},
year = {2004}
}
@article{Widmer2018,
abstract = {Comment sections of editorial news sites, blogs, and forums are becoming a increasingly popular place for users and consumers to exchange their opinions. In this thesis we perform topic modeling with Latent Dirichlet Allocation (LDA) and Non-negative Matrix factorization to identify the main topics of German and U.S. discussions regarding organic food. With the help of a domain expert the identified topics were compared to consumer responses from qualitative surveys. Our results show that the issues that are important to the consumers when making consumption decisions are also expressed in online discussions. Further, we show that topic modeling can be used to identify events and trends that had an significant impact on online discussions.},
author = {Widmer, Christian},
file = {:C$\backslash$:/Users/Maria/Desktop/thesis{\_}ONLINE.pdf:pdf},
title = {{Topic Modeling for Opinion Mining}},
year = {2018}
}
